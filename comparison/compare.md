| Metric              | **Ragas**                                           | **DeepEval**                                       |
|----------------------|------------------------------------------------------|-----------------------------------------------------|
| **Faithfulness**      | Measures if the **answer only states things found in the retrieved context.** <br> Ensures no hallucinations beyond the context. <br>Uses LLM-as-judge or entailment checks. | Measures if the **answer stays consistent with the retrieval context.** <br> Checks if claims can be traced back to context, using similarity or LLM judgment. |
| **Context Precision** | Measures **how much of the retrieved context was actually necessary and relevant to generate the answer.** <br> High precision = answer focuses on key details, ignores unrelated context. | Checks whether the **most important context is accurately reflected in the answer**, compared to expected output. <br> Needs `expected_output`. Measures: “Did the answer extract precisely what it needed?” |
| **Context Recall**    | Measures if the **answer included all critical points from the retrieved context needed to fully answer the question.** <br>Low recall means it missed key context details. |  Not a standard DeepEval metric. Typically approximated via faithfulness + relevancy or a custom `GEval` check. |
| **Answer Relevancy**  | Measures if the **answer is directly relevant to the original question**, often by prompting an LLM judge. <br>Penalizes rambling or off-topic answers. | Also measures **if the answer addresses the question**, usually by comparing `actual_output` to `expected_output`. |
| **Correctness / GEval** |  Not directly supported. <br>Usually done via custom prompt engineering. |  Uses `GEval` to define **custom criteria in natural language**, e.g. “Does the answer correctly explain why leaves change color in fall?” |
